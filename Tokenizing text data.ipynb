{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOhrb4prosSvTS2oGsel1MF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8aadrXLnwqrR","executionInfo":{"status":"aborted","timestamp":1668709587138,"user_tz":-330,"elapsed":3,"user":{"displayName":"Umesh Sonar","userId":"03420915676304825229"}}},"outputs":[],"source":["pip install nltk\n","pip install gensim\n","pip install pattern"]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"znwIvxNsxrao","outputId":"e872c031-c56a-45eb-e128-681dc12f5245","executionInfo":{"status":"ok","timestamp":1668709605423,"user_tz":-330,"elapsed":1548,"user":{"displayName":"Umesh Sonar","userId":"03420915676304825229"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["#Tokenizing text data\n","from nltk.tokenize import sent_tokenize, \\\n","        word_tokenize, WordPunctTokenizer"],"metadata":{"id":"08QlrbQRw-AT","executionInfo":{"status":"ok","timestamp":1668709609647,"user_tz":-330,"elapsed":2,"user":{"displayName":"Umesh Sonar","userId":"03420915676304825229"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Define input text\n","input_text = \"Do you know how tokenization works? It's actually  quite interesting! Let's analyze a couple of sentences and  figure it out.\""],"metadata":{"id":"B6o5hbMQw-B4","executionInfo":{"status":"ok","timestamp":1668709610344,"user_tz":-330,"elapsed":7,"user":{"displayName":"Umesh Sonar","userId":"03420915676304825229"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#Divide the input text into sentence tokens:\n","# Sentence tokenizer \n","print(\"\\nSentence tokenizer:\")\n","print(sent_tokenize(input_text))"],"metadata":{"id":"EMtjYlYSw-Gm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668709610944,"user_tz":-330,"elapsed":10,"user":{"displayName":"Umesh Sonar","userId":"03420915676304825229"}},"outputId":"6025acb8-07c7-4bfe-bfb5-1f295de51739"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Sentence tokenizer:\n","['Do you know how tokenization works?', \"It's actually  quite interesting!\", \"Let's analyze a couple of sentences and  figure it out.\"]\n"]}]},{"cell_type":"code","source":["#Divide the input text into word tokens:\n","# Word tokenizer\n","print(\"\\nWord tokenizer:\")\n","print(word_tokenize(input_text))"],"metadata":{"id":"nBaz4nRlxJ1j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668709612012,"user_tz":-330,"elapsed":8,"user":{"displayName":"Umesh Sonar","userId":"03420915676304825229"}},"outputId":"9cae4eb9-9a6e-465f-ebca-96fff5037768"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Word tokenizer:\n","['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'s\", 'actually', 'quite', 'interesting', '!', 'Let', \"'s\", 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n"]}]},{"cell_type":"code","source":["#Divide the input text into word tokens using the WordPunct tokenizer:\n","# WordPunct tokenizer\n","print(\"\\nWord punct tokenizer:\")\n","print(WordPunctTokenizer().tokenize(input_text))"],"metadata":{"id":"JFFoX421xJ3G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668709612012,"user_tz":-330,"elapsed":4,"user":{"displayName":"Umesh Sonar","userId":"03420915676304825229"}},"outputId":"a1942154-63a1-489b-d297-77d3575dcaea"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Word punct tokenizer:\n","['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'\", 's', 'actually', 'quite', 'interesting', '!', 'Let', \"'\", 's', 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Kn9UAK3hxUE0","executionInfo":{"status":"aborted","timestamp":1668709587145,"user_tz":-330,"elapsed":10,"user":{"displayName":"Umesh Sonar","userId":"03420915676304825229"}}},"execution_count":null,"outputs":[]}]}